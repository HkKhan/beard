# BeardAR System Architecture Summary

## System Overview
BeardAR is an AI-powered facial hair segmentation and augmented reality overlay system. It uses Meta's Segment Anything Model (SAM) with fine-tuned weights for beard detection, combined with MediaPipe face landmark detection for precise spatial alignment.

## Core Components

### Frontend (React/TypeScript)
- **FaceTracker**: MediaPipe-based face landmark detection (468 points)
- **ScanView**: Multi-angle face capture with real-time feedback
- **ProjectView**: Live beard overlay with adjustable visualization
- **State Management**: Zustand store for templates and session data

### Backend (FastAPI/Python)
- **SAM Model**: Fine-tuned VIT-B architecture for beard segmentation
- **Face Processing**: Landmark-based prompt generation for SAM
- **Template System**: Canonical space accumulation and warping
- **Frame Storage**: Persistent scan data with metadata

### Data Flow Architecture

## Phase 1: Face Scanning & Capture

### User Interaction Flow
1. User initiates scan → ScanView component loads
2. FaceTracker initializes MediaPipe FaceMesh
3. User follows on-screen instructions (center, left, right, up, down rotations)
4. System captures 60 frames at 100ms intervals during 6-second scan

### Frame Processing Pipeline
```
Video Frame → Face Detection → Landmark Extraction → SAM Segmentation → Mask Accumulation
     ↓              ↓                ↓                  ↓                  ↓
  HTML5 Canvas → MediaPipe → 468-point mesh → Beard prompts → Binary mask → Weighted sum
```

### Technical Details
- **Face Landmarks**: 468 normalized coordinates (x,y,z) per frame
- **SAM Prompts**: Generated from jawline, chin, and neck landmarks
- **Mask Accumulation**: Confidence-weighted masks warped to 512x512 canonical space
- **Storage**: Frames saved as JSON with base64 images, landmarks, and SAM masks

## Phase 2: Template Creation & Finalization

### Batch Processing
1. Load saved frames from disk storage
2. Re-run SAM segmentation if needed (fallback for missing masks)
3. Accumulate masks with confidence weighting and edge fading
4. Apply morphological smoothing and contour extraction

### Template Generation Algorithm
```
Saved Frames → SAM Inference → Canonical Warping → Weighted Accumulation → Morphological Processing → Contour Extraction
     ↓               ↓                  ↓                      ↓                       ↓                      ↓
  JSON data → VIT-B model → Affine transform → Gaussian blur → Binary threshold → OpenCV findContours
```

### Key Parameters
- **Canonical Size**: 512x512 pixels
- **Threshold**: 0.4 (probability cutoff for binary mask)
- **Morphology**: 11x11 ellipse kernel for gap filling
- **Contour**: Simplified using Douglas-Peucker algorithm

## Phase 3: Real-time Projection

### Live Segmentation Approach
1. Capture current video frame (640x480 JPEG)
2. Extract face landmarks from current pose
3. Generate beard-specific prompts from landmarks
4. Run SAM inference on current frame
5. Extract contour and render overlay

### Projection Pipeline
```
Live Video → Face Tracking → Prompt Generation → SAM Inference → Contour Rendering
     ↓             ↓                ↓                  ↓               ↓
 HTML5 Video → MediaPipe → Landmark-based → VIT-B model → Canvas overlay
```

### Performance Optimizations
- **Throttling**: 100ms intervals (SAM inference ~80-120ms)
- **Caching**: Contour points cached between frames
- **Resolution**: 640x480 input, real-time processing
- **Fallback**: Template projection available for low-power devices

## Technical Architecture

### Model Architecture
- **Base Model**: Meta SAM VIT-B (91M parameters)
- **Fine-tuning**: Custom beard dataset (COCO format, 1000+ images)
- **Input**: 1024x1024 RGB images with point/box prompts
- **Output**: Multi-mask predictions with confidence scores

### API Endpoints
- `POST /segment/live`: Real-time SAM segmentation
- `POST /template/add-frame`: Frame accumulation during scan
- `POST /template/finalize`: Template creation from frames
- `POST /scans/{id}/finalize`: Batch processing saved scans
- `POST /scans/{id}/load`: Load saved frame data

### Data Storage
- **Templates**: In-memory with optional persistence
- **Scans**: JSON files with base64 images and metadata
- **Models**: Cached in GPU memory during session
- **State**: LocalStorage for UI preferences

## Performance Characteristics

### Latency Breakdown
- **Face Detection**: 15-25ms (MediaPipe)
- **SAM Inference**: 80-120ms (GPU), 200-400ms (CPU)
- **Template Projection**: <5ms (affine warp)
- **Total Projection**: 100-150ms end-to-end

### Memory Usage
- **Model**: ~350MB GPU memory (VIT-B)
- **Frame Buffer**: ~2MB per stored frame
- **Template**: ~256KB compressed PNG
- **Working Set**: 50-100 frames during scanning

### Accuracy Metrics
- **Face Detection**: >95% detection rate
- **SAM mIoU**: 0.85+ on beard segmentation
- **Template Quality**: Improved with multi-angle scanning
- **Real-time Tracking**: 10-15 FPS on modern hardware

## System Requirements
- **Frontend**: Modern browser with WebGL, MediaPipe WASM
- **Backend**: Python 3.8+, PyTorch, CUDA-compatible GPU
- **Network**: 10-50 Mbps for real-time operation
- **Storage**: 500MB for models, variable for scan data

## Key Innovations
1. **Live vs Template**: Dynamic choice between fast projection and accurate segmentation
2. **Prompt Engineering**: Landmark-based SAM prompting for consistent beard detection
3. **Multi-angle Accumulation**: Confidence-weighted mask fusion across poses
4. **Real-time Pipeline**: Optimized for 10+ FPS overlay rendering
5. **Fallback Resilience**: Multiple processing paths for reliability

## Future Architecture Considerations
- **Edge Deployment**: ONNX model conversion for mobile inference
- **Streaming**: WebRTC for remote processing pipelines
- **Multi-user**: Database-backed template storage and sharing
- **Advanced Models**: Integration with SAM-2 or other foundation models
- **Performance**: TensorRT optimization for lower latency
